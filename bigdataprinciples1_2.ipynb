{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions Big Data Principles\n",
    "\n",
    "\n",
    "\n",
    "### Chapter 1\n",
    "\n",
    "1.- Mention an advantage and a disadvantage of using Hadoop as your computational sytem to handle large amounts of data. Do you think that Elasticsearch could help with the disadvantage? \n",
    "- R.- It can use large amount of data and make their process paralleized, the disadvantage is that it computes with high latancy. No because the elasticsearch engines would help in the visuzalization part, not before\n",
    "\n",
    "2.- Imaine that you have de dumbest personal in your company and they have lots of executioanl erros, which property need to be changed in order to have a human mistake tolerance. Why?\n",
    "- R. it needs to be inmutable, because if it is inmutable you are only having bar registers, otherwise they would be damaging the entire data\n",
    "\n",
    "3.- In the whole chapter 1, a concept is repeted several times, it is very important for scalability... which \"concept\" are we talking about? what would be the natural funciton of this?\n",
    "- R.- Lambda Architecture, to implement any function on any data. \n",
    "\n",
    "4.- According to the book which theorem states that a service which is availability and full consistency?, name the characteristics of the theorem.\n",
    "\n",
    "- R.-CAP- CONSISTENCY, AVAILABILIT, PARTITION TOLERANCE.  choose 2 and forget about the otherone. \n",
    "\n",
    "5.- Lets say that a business needs consistency and partition tolerance. which platform would you use? \n",
    "- R.-Mongo DB, HBase, Redis, Memcache\n",
    "\n",
    "6.- Lets say that you are working with a hospital in this COVID-19 situation, it is a small hospital, which characteristics of the theorem that we saw before would you take? \n",
    "- R.- Availability and Consistency\n",
    "\n",
    "7.- If you want a tool or system which can handle failiors at the software layer, which computaional system would you use?\n",
    "- R.- MapReduce \n",
    "\n",
    "8.- Taking in count the question number 7, would you say that this \"system\" is implemented in elasticsearch? would you say that it makes this process when ir reach Kibana, or before reaching kibana?\n",
    "- R.- It may not be the same mapping, but it is likely, and it would be before reaching Kibana. \n",
    "\n",
    "## Part 1\n",
    "\n",
    "\n",
    "### Chapter 2 \n",
    "\n",
    "1 If your boss asked you to delate some data and it is not a wide dataset, what would you do?? What would be your next step?\n",
    "- R.- Create a second copy... the second question is optional, but you shoud replace it after filtraiting. \n",
    "\n",
    "2 The book states to different \"structures\" that can be used to represantate data. Give the name of these \"structures\", and state which one are we using in the last week activities.\n",
    "- R.- XML, JSON. The structuree that we are using is a JSON format.\n",
    "\n",
    "3 What is the difference between structured and semistructured data? Can you fing another profile? Name it and tell the characteristics. \n",
    "- R.- Structured data is the one that can be addressable, it is also organized, an example could be the relational data. the semistructures are the ones such as XML and JSON, it is the data that cannot be in a database because of the lack of fields, it can be analyzed and it has keys, can be turned into structured data. the unstrutured data\n",
    "\n",
    "4 In chapter 2 they talk about graph schemas, but in chapter 1 they mention what happend with data if there is any anomalie or at introducing data. what would be a complication(mentioned)....? what process does it take in count when you input new data(according to chapter 1)? \n",
    "- R.- A bad input of a register, a timestamp for example. It does a rebalance onto the new nodes\n",
    "\n",
    "5 State the 4 principal advantages of the big data paradigm, also explain why are they advantages\n",
    "- R.- Is queryable at any time in its history, tolerates human errors, handles partial information, has the advantages of both normalized and denormalized forms. It is accecible whenever you want, the mistakes from humans are not very worring because it is inmutables so you only have to delate, it can be fill with NULL's automatically, finally, it is devided into two different sections what let you extract the best characteristics of each. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
